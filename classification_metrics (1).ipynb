{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_metrics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKnPMgxtAThD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**1. Confusion Matrix:** \n",
        "\n",
        "\n",
        "A confusion matrix is an N X N matrix, where N is the number of classes being predicted.\n",
        "\n",
        "![alt text](https://h2oai.github.io/tutorials/lms/machine-learning-experiment-scoring-and-analysis-tutorial-financial-focus/img/274b9cf9f5f666c6.jpg)\n",
        "\n",
        "True Positives (TP): True positives are the cases when the actual class of the data point was 1(True) and the predicted is also 1(True).\n",
        "\n",
        "Ex: The case where a person is actually having cancer(1) and the model classifying his case as cancer(1) comes under True positive.\n",
        "\n",
        "True Negatives (TN): True negatives are the cases when the actual class of the data point was 0(False) and the predicted is also 0(False).\n",
        "\n",
        "Ex: The case where a person NOT having cancer and the model classifying his case as Not cancer comes under True Negatives.\n",
        "\n",
        "False Positives (FP): False positives are the cases when the actual class of the data point was 0(False) and the predicted is 1(True).\n",
        "\n",
        "Ex: A person NOT having cancer and the model classifying his case as cancer comes under False Positives.\n",
        "\n",
        "False Negatives (FN): False negatives are the cases when the actual class of the data point was 1(True) and the predicted is 0(False). \n",
        "\n",
        "Ex: A person having cancer and the model classifying his case as No-cancer comes under False Negatives\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdMeestKATqr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**A. Accuracy:**\n",
        "\n",
        "Accuracy in classification problems is the number of correct predictions made by the model over all kinds predictions made.\n",
        "\n",
        "**Accuracy = (TP+TN)/(TP+FN+FP+TN)**\n",
        "\n",
        "Accuracy is a good measure when the target variable classes in the data are nearly balanced.\n",
        "\n",
        "**B. Precision**\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1872/1*pOtBHai4jFd-ujaNXPilRg.png)\n",
        "\n",
        "Out of all the positive classes we have predicted correctly, how many are actually positive.\n",
        "it is measured from 0.0 to 1.0, where 0.0 is the worst and 1.0 is the best precision. Precision is more focused on the positive class than in the negative class, it actually measures the probability of correct detection of positive values (TP and FP).\n",
        "\n",
        "what proportion of patients that we diagnosed as having cancer, actually had cancer.\n",
        "\n",
        "**C. Recall/Sensitivity/TPR:**\n",
        "\n",
        "\n",
        "Out of all the positive classes, how much we predicted correctly. It should be high as possible.\n",
        "\n",
        "It tells us how many correct positive results occurred from all the positive samples available during the test of the model.\n",
        "\n",
        "\n",
        "\n",
        "tells us what proportion of patients that actually had cancer was diagnosed by the algorithm as having cancer.\n",
        "\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/11baf9ea1410e9d4a685e4a543f22b9890a6f716/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f3535322f312a755230397a546c5067496a3550764d594a5a536356672e706e67)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNGrPnFYAT7Z",
        "colab_type": "text"
      },
      "source": [
        "**D. F-Score:** \n",
        "\n",
        "The F-Measure is a popular metric for imbalanced classification.\n",
        "\n",
        "![alt text](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/Screenshot-2019-05-14-at-12.34.35-PM.png)\n",
        "\n",
        "**E. Specificity/TNR:** \n",
        "\n",
        "Specificity is a measure for how well the model is predicting for the negative case correctly\n",
        "\n",
        "Specificity = **True Negative Rate** = TN / (FP + TN)\n",
        "\n",
        "1 - Specificity = **False Positive Rate** = 1 - True Negative Rate = FP / (FP + TN )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9N1HI-fMtfa",
        "colab_type": "text"
      },
      "source": [
        "Examples: \n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*uLbVblrwaqf1-sVT5A4TRg.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*JJ_AEptV8jF7bu17zuVxLg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmwOOqWZJ5Mx",
        "colab_type": "text"
      },
      "source": [
        "**2. ROC Curve(Receiver Operating Characteristic)**\n",
        "\n",
        "\n",
        "A ROC curve is a diagnostic plot for summarizing the behavior of a model by calculating the false positive rate and true positive rate for a set of predictions by the model under different thresholds.\n",
        "\n",
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/10/Depiction-of-a-ROC-Curve.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9av2sUxLDCh",
        "colab_type": "text"
      },
      "source": [
        "A ROC curve is a useful tool because it only focuses on how well the model was able to distinguish between classes\n",
        "\n",
        "**3. Precision-Recall Curves**\n",
        "\n",
        "ROC curves should be used when there are roughly equal numbers of observations for each class.\n",
        "\n",
        "Precision-Recall curves should be used when there is a moderate to large class imbalance.\n",
        "\n",
        "Prec-Recall curve plots the precision or positive predictive value (y-axis) versus sensitivity/TNR/Recall(x-axis) for every possible classification threshold.\n",
        "\n",
        "The Pre-Recall plot is broken down into two sections, \"Good\" and \"Poor\" performance. \"Good\" performance can be found on the upper right corner of the plot and \"Poor\" performance on the lower left corner, \n",
        "\n",
        ".This division is generated by the baseline. The baseline for Prec-Recall is determined by the ratio of Positives(P) and Negatives(N), where y = P/(P+N), this function represents a classifier with a random performance level. When the dataset is balanced, the value of the baseline is y = 0.5. If the dataset is imbalanced where the number of P's is higher than N's then the baseline will be adjusted accordingly and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chib6myI0JVm",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://besjournals.onlinelibrary.wiley.com/cms/asset/cfdd78e4-fe42-40b3-bc3b-51480b1af5de/mee313140-fig-0001-m.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlKk1mF32HXf",
        "colab_type": "text"
      },
      "source": [
        "**4. MCC**\n",
        "\n",
        "The Matthews Correlation Coefficient (MCC) has a range of -1 to 1 where -1 indicates a completely wrong binary classifier while 1 indicates a completely correct binary classifier.\n",
        "\n",
        "\n",
        "![alt text](https://pubs.rsc.org/image/article/2015/ay/c5ay00168d/c5ay00168d-t15_hi-res.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAfTs5HtGZlH",
        "colab_type": "text"
      },
      "source": [
        "**5. Cumulative Gain and Lift**\n",
        "\n",
        "\n",
        "Gain or lift is a measure of the effectiveness of a classification model calculated as the ratio between the results obtained with and without the model. \n",
        "\n",
        "confusion matrix that evaluates models on the whole population gain or lift chart evaluates model performance in a portion of the population\n",
        "\n",
        "In the cumulative gains chart, the x-axis shows the percentage of cases from the total number of cases in the test dataset, while the y-axis shows the percentage of positive responses in terms of quantiles.\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://4.bp.blogspot.com/-Jf5biFHVzvc/UAMvtA1OJQI/AAAAAAAAAfc/HgprZ5i4X9Y/s320/Comparative+Response+Table.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtQI-5PlJLU6",
        "colab_type": "text"
      },
      "source": [
        "The y-axis shows the percentage of positive responses. This is a percentage of the total possible positive responses (20,000 as the overall response rate shows).\n",
        "\n",
        "The x-axis shows the percentage of customers contacted, which is a fraction of the 100,000 total customers.\n",
        "\n",
        "Baseline (overall response rate): If we contact X% of customers then we will receive X% of the total positive responses.\n",
        "\n",
        "Lift Curve: Using the predictions of the response model, calculate the percentage of positive responses for the percent of customers contacted and map these points to create the lift curve.\n",
        "\n",
        "\n",
        "![alt text](https://www3.nd.edu/~busiforc/images/Lift_Confusion_Chart/cumulative%20gains%20.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaFoFt06JLhz",
        "colab_type": "text"
      },
      "source": [
        "To plot Lift chart: Calculate the points on the lift curve by determining the ratio between the result predicted by our model and the result using no model.\n",
        "\n",
        "Example: For contacting 10% of customers, using no model we should get 10% of responders and using the given model we should get 30% of responders. The y-value of the lift curve at 10% is 30 / 10 = 3.\n",
        "\n",
        "\n",
        "**Lift = Predictive rate/ Actual rate**\n",
        "\n",
        "![alt text](http://www2.cs.uregina.ca/~dbd/cs831/notes/lift_chart/lift_chart.gif)\n",
        "\n",
        "Analyzing the Charts: Cumulative gains and lift charts are a graphical representation of the advantage of using a predictive model to choose which customers to contact. The lift chart shows how much more likely we are to receive respondents than if we contact a random sample of customers. For example, by contacting only 10% of customers based on the predictive model we will reach 3 times as many respondents as if we use no model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoCoArP8MNV_",
        "colab_type": "text"
      },
      "source": [
        "**6. K-s Chart**\n",
        "\n",
        "\n",
        "K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions.\n",
        "\n",
        "The K-S is 100 if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives. On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. \n",
        "\n",
        "In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDk44CztMNfZ",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://h2oai.github.io/tutorials/lms/machine-learning-experiment-scoring-and-analysis-tutorial-financial-focus/img/37b63aaa04b96ef7.jpg)\n",
        "\n",
        "**Step 1:** Once the prediction probability scores are obtained, the observations are sorted by decreasing order of probability scores. This way, you can expect the rows at the top to be classified as 1 while rows at the bottom to be 0's.\n",
        "\n",
        "**Step 2:** All observations are then split into 10 equal sized buckets (bins).\n",
        "\n",
        "**Step 3:** Then, KS statistic is the maximum difference between the cumulative percentage of responders or 1's (cumulative true positive rate) and cumulative percentage of non-responders or 0's (cumulative false positive rate).\n",
        "\n",
        "\n",
        "The significance of KS statistic is, it helps to understand, what portion of the population should be targeted to get the highest response rate (1's)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph2ymUzTPznk",
        "colab_type": "text"
      },
      "source": [
        "**7. Log loss:** \n",
        "\n",
        "\n",
        "The logarithmic loss metric can be used to evaluate the performance of a binomial or multinomial classifier. Unlike AUC which looks at how well a model can classify a binary target, logloss evaluates how close a model's predicted values (uncalibrated probability estimates) are to the actual target value. For example, does a model tend to assign a high predicted value like .80 for the positive class, or does it show a poor ability to recognize the positive class and assign a lower predicted value like .50? A model with a log loss of 0 would be the perfect classifier. When the model is unable to make correct predictions, the log loss increases making the model a poor model.\n",
        "\n",
        "\n",
        "**Binary classification equation:**\n",
        "\n",
        "\n",
        "![alt text](https://h2oai.github.io/tutorials/lms/machine-learning-experiment-scoring-and-analysis-tutorial-financial-focus/img/10f8d5c84bee475.jpg)\n",
        "\n",
        "\n",
        "**Multiclass classification equation:**\n",
        "\n",
        "![alt text](https://h2oai.github.io/tutorials/lms/machine-learning-experiment-scoring-and-analysis-tutorial-financial-focus/img/19e5d3407864e1cf.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtca1j4nPz3A",
        "colab_type": "text"
      },
      "source": [
        "Where:\n",
        "\n",
        "N is the total number of rows (observations) of your corresponding dataframe.\n",
        "\n",
        "w is the per row user-defined weight (defaults is 1).\n",
        "\n",
        "C is the total number of classes (C=2 for binary classification).\n",
        "\n",
        "p is the predicted value (uncalibrated probability) assigned to a given row (observation).\n",
        "\n",
        "y is the actual target value.\n",
        "\n",
        "\n",
        "The graph below shows the range of possible log loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications that are confident and wrong!\n",
        "\n",
        "\n",
        "![alt text](http://wiki.fast.ai/images/4/43/Log_loss_graph.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb-HZOjJRXOb",
        "colab_type": "text"
      },
      "source": [
        "**8.  Gini coefficient**\n",
        "\n",
        "\n",
        "Gini coefficient is sometimes used in classification problems. Gini coefficient can be straigh away derived from the AUC ROC number. Gini is nothing but ratio between area between the ROC curve and the diagnol line & the area of the above triangle. Following is the formulae used :\n",
        "\n",
        "Gini = 2*AUC â€“ 1\n",
        "\n",
        "Gini above 60% is a good model. For the case in hand we get Gini as 92.7%."
      ]
    }
  ]
}